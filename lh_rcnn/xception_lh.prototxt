layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 512
      dim: 512
    }
  }
}
layer {
  name: "255"
  type: "Convolution"
  bottom: "data"
  top: "255"
  convolution_param {
    num_output: 32
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "256_bn"
  type: "BatchNorm"
  bottom: "255"
  top: "256"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "256"
  type: "Scale"
  bottom: "256"
  top: "256"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "257"
  type: "ReLU"
  bottom: "256"
  top: "257"
}
layer {
  name: "258"
  type: "Convolution"
  bottom: "257"
  top: "258"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "259_bn"
  type: "BatchNorm"
  bottom: "258"
  top: "259"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "259"
  type: "Scale"
  bottom: "259"
  top: "259"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "260"
  type: "ReLU"
  bottom: "259"
  top: "260"
}
layer {
  name: "261"
  type: "Convolution"
  bottom: "260"
  top: "261"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 64
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "262"
  type: "Convolution"
  bottom: "261"
  top: "262"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "263_bn"
  type: "BatchNorm"
  bottom: "262"
  top: "263"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "263"
  type: "Scale"
  bottom: "263"
  top: "263"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "264"
  type: "ReLU"
  bottom: "263"
  top: "264"
}
layer {
  name: "265"
  type: "Convolution"
  bottom: "264"
  top: "265"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "266"
  type: "Convolution"
  bottom: "265"
  top: "266"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "267_bn"
  type: "BatchNorm"
  bottom: "266"
  top: "267"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "267"
  type: "Scale"
  bottom: "267"
  top: "267"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "268"
  type: "Pooling"
  bottom: "267"
  top: "268"
  pooling_param {
    pool: MAX
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    pad_h: 1
    pad_w: 1
  }
}
layer {
  name: "269"
  type: "Convolution"
  bottom: "260"
  top: "269"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "270_bn"
  type: "BatchNorm"
  bottom: "269"
  top: "270"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "270"
  type: "Scale"
  bottom: "270"
  top: "270"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "271"
  type: "Eltwise"
  bottom: "268"
  bottom: "270"
  top: "271"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "272"
  type: "ReLU"
  bottom: "271"
  top: "272"
}
layer {
  name: "273"
  type: "Convolution"
  bottom: "272"
  top: "273"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 128
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "274"
  type: "Convolution"
  bottom: "273"
  top: "274"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "275_bn"
  type: "BatchNorm"
  bottom: "274"
  top: "275"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "275"
  type: "Scale"
  bottom: "275"
  top: "275"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "276"
  type: "ReLU"
  bottom: "275"
  top: "276"
}
layer {
  name: "277"
  type: "Convolution"
  bottom: "276"
  top: "277"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 256
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "278"
  type: "Convolution"
  bottom: "277"
  top: "278"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "279_bn"
  type: "BatchNorm"
  bottom: "278"
  top: "279"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "279"
  type: "Scale"
  bottom: "279"
  top: "279"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "280"
  type: "Pooling"
  bottom: "279"
  top: "280"
  pooling_param {
    pool: MAX
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    pad_h: 1
    pad_w: 1
  }
}
layer {
  name: "281"
  type: "Convolution"
  bottom: "271"
  top: "281"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "282_bn"
  type: "BatchNorm"
  bottom: "281"
  top: "282"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "282"
  type: "Scale"
  bottom: "282"
  top: "282"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "283"
  type: "Eltwise"
  bottom: "280"
  bottom: "282"
  top: "283"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "284"
  type: "ReLU"
  bottom: "283"
  top: "284"
}
layer {
  name: "285"
  type: "Convolution"
  bottom: "284"
  top: "285"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 256
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "286"
  type: "Convolution"
  bottom: "285"
  top: "286"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "287_bn"
  type: "BatchNorm"
  bottom: "286"
  top: "287"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "287"
  type: "Scale"
  bottom: "287"
  top: "287"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "288"
  type: "ReLU"
  bottom: "287"
  top: "288"
}
layer {
  name: "289"
  type: "Convolution"
  bottom: "288"
  top: "289"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "290"
  type: "Convolution"
  bottom: "289"
  top: "290"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "291_bn"
  type: "BatchNorm"
  bottom: "290"
  top: "291"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "291"
  type: "Scale"
  bottom: "291"
  top: "291"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "292"
  type: "Pooling"
  bottom: "291"
  top: "292"
  pooling_param {
    pool: MAX
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    pad_h: 1
    pad_w: 1
  }
}
layer {
  name: "293"
  type: "Convolution"
  bottom: "283"
  top: "293"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "294_bn"
  type: "BatchNorm"
  bottom: "293"
  top: "294"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "294"
  type: "Scale"
  bottom: "294"
  top: "294"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "295"
  type: "Eltwise"
  bottom: "292"
  bottom: "294"
  top: "295"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "296"
  type: "ReLU"
  bottom: "295"
  top: "296"
}
layer {
  name: "297"
  type: "Convolution"
  bottom: "296"
  top: "297"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "298"
  type: "Convolution"
  bottom: "297"
  top: "298"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "299_bn"
  type: "BatchNorm"
  bottom: "298"
  top: "299"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "299"
  type: "Scale"
  bottom: "299"
  top: "299"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "300"
  type: "ReLU"
  bottom: "299"
  top: "300"
}
layer {
  name: "301"
  type: "Convolution"
  bottom: "300"
  top: "301"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "302"
  type: "Convolution"
  bottom: "301"
  top: "302"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "303_bn"
  type: "BatchNorm"
  bottom: "302"
  top: "303"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "303"
  type: "Scale"
  bottom: "303"
  top: "303"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "304"
  type: "ReLU"
  bottom: "303"
  top: "304"
}
layer {
  name: "305"
  type: "Convolution"
  bottom: "304"
  top: "305"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "306"
  type: "Convolution"
  bottom: "305"
  top: "306"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "307_bn"
  type: "BatchNorm"
  bottom: "306"
  top: "307"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "307"
  type: "Scale"
  bottom: "307"
  top: "307"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "308"
  type: "Eltwise"
  bottom: "307"
  bottom: "295"
  top: "308"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "309"
  type: "ReLU"
  bottom: "308"
  top: "309"
}
layer {
  name: "310"
  type: "Convolution"
  bottom: "309"
  top: "310"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "311"
  type: "Convolution"
  bottom: "310"
  top: "311"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "312_bn"
  type: "BatchNorm"
  bottom: "311"
  top: "312"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "312"
  type: "Scale"
  bottom: "312"
  top: "312"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "313"
  type: "ReLU"
  bottom: "312"
  top: "313"
}
layer {
  name: "314"
  type: "Convolution"
  bottom: "313"
  top: "314"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "315"
  type: "Convolution"
  bottom: "314"
  top: "315"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "316_bn"
  type: "BatchNorm"
  bottom: "315"
  top: "316"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "316"
  type: "Scale"
  bottom: "316"
  top: "316"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "317"
  type: "ReLU"
  bottom: "316"
  top: "317"
}
layer {
  name: "318"
  type: "Convolution"
  bottom: "317"
  top: "318"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "319"
  type: "Convolution"
  bottom: "318"
  top: "319"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "320_bn"
  type: "BatchNorm"
  bottom: "319"
  top: "320"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "320"
  type: "Scale"
  bottom: "320"
  top: "320"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "321"
  type: "Eltwise"
  bottom: "320"
  bottom: "308"
  top: "321"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "322"
  type: "ReLU"
  bottom: "321"
  top: "322"
}
layer {
  name: "323"
  type: "Convolution"
  bottom: "322"
  top: "323"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "324"
  type: "Convolution"
  bottom: "323"
  top: "324"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "325_bn"
  type: "BatchNorm"
  bottom: "324"
  top: "325"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "325"
  type: "Scale"
  bottom: "325"
  top: "325"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "326"
  type: "ReLU"
  bottom: "325"
  top: "326"
}
layer {
  name: "327"
  type: "Convolution"
  bottom: "326"
  top: "327"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "328"
  type: "Convolution"
  bottom: "327"
  top: "328"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "329_bn"
  type: "BatchNorm"
  bottom: "328"
  top: "329"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "329"
  type: "Scale"
  bottom: "329"
  top: "329"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "330"
  type: "ReLU"
  bottom: "329"
  top: "330"
}
layer {
  name: "331"
  type: "Convolution"
  bottom: "330"
  top: "331"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "332"
  type: "Convolution"
  bottom: "331"
  top: "332"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "333_bn"
  type: "BatchNorm"
  bottom: "332"
  top: "333"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "333"
  type: "Scale"
  bottom: "333"
  top: "333"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "334"
  type: "Eltwise"
  bottom: "333"
  bottom: "321"
  top: "334"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "335"
  type: "ReLU"
  bottom: "334"
  top: "335"
}
layer {
  name: "336"
  type: "Convolution"
  bottom: "335"
  top: "336"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "337"
  type: "Convolution"
  bottom: "336"
  top: "337"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "338_bn"
  type: "BatchNorm"
  bottom: "337"
  top: "338"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "338"
  type: "Scale"
  bottom: "338"
  top: "338"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "339"
  type: "ReLU"
  bottom: "338"
  top: "339"
}
layer {
  name: "340"
  type: "Convolution"
  bottom: "339"
  top: "340"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "341"
  type: "Convolution"
  bottom: "340"
  top: "341"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "342_bn"
  type: "BatchNorm"
  bottom: "341"
  top: "342"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "342"
  type: "Scale"
  bottom: "342"
  top: "342"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "343"
  type: "ReLU"
  bottom: "342"
  top: "343"
}
layer {
  name: "344"
  type: "Convolution"
  bottom: "343"
  top: "344"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "345"
  type: "Convolution"
  bottom: "344"
  top: "345"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "346_bn"
  type: "BatchNorm"
  bottom: "345"
  top: "346"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "346"
  type: "Scale"
  bottom: "346"
  top: "346"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "347"
  type: "Eltwise"
  bottom: "346"
  bottom: "334"
  top: "347"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "348"
  type: "ReLU"
  bottom: "347"
  top: "348"
}
layer {
  name: "349"
  type: "Convolution"
  bottom: "348"
  top: "349"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "350"
  type: "Convolution"
  bottom: "349"
  top: "350"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "351_bn"
  type: "BatchNorm"
  bottom: "350"
  top: "351"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "351"
  type: "Scale"
  bottom: "351"
  top: "351"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "352"
  type: "ReLU"
  bottom: "351"
  top: "352"
}
layer {
  name: "353"
  type: "Convolution"
  bottom: "352"
  top: "353"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "354"
  type: "Convolution"
  bottom: "353"
  top: "354"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "355_bn"
  type: "BatchNorm"
  bottom: "354"
  top: "355"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "355"
  type: "Scale"
  bottom: "355"
  top: "355"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "356"
  type: "ReLU"
  bottom: "355"
  top: "356"
}
layer {
  name: "357"
  type: "Convolution"
  bottom: "356"
  top: "357"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "358"
  type: "Convolution"
  bottom: "357"
  top: "358"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "359_bn"
  type: "BatchNorm"
  bottom: "358"
  top: "359"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "359"
  type: "Scale"
  bottom: "359"
  top: "359"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "360"
  type: "Eltwise"
  bottom: "359"
  bottom: "347"
  top: "360"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "361"
  type: "ReLU"
  bottom: "360"
  top: "361"
}
layer {
  name: "362"
  type: "Convolution"
  bottom: "361"
  top: "362"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "363"
  type: "Convolution"
  bottom: "362"
  top: "363"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "364_bn"
  type: "BatchNorm"
  bottom: "363"
  top: "364"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "364"
  type: "Scale"
  bottom: "364"
  top: "364"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "365"
  type: "ReLU"
  bottom: "364"
  top: "365"
}
layer {
  name: "366"
  type: "Convolution"
  bottom: "365"
  top: "366"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "367"
  type: "Convolution"
  bottom: "366"
  top: "367"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "368_bn"
  type: "BatchNorm"
  bottom: "367"
  top: "368"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "368"
  type: "Scale"
  bottom: "368"
  top: "368"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "369"
  type: "ReLU"
  bottom: "368"
  top: "369"
}
layer {
  name: "370"
  type: "Convolution"
  bottom: "369"
  top: "370"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "371"
  type: "Convolution"
  bottom: "370"
  top: "371"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "372_bn"
  type: "BatchNorm"
  bottom: "371"
  top: "372"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "372"
  type: "Scale"
  bottom: "372"
  top: "372"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "373"
  type: "Eltwise"
  bottom: "372"
  bottom: "360"
  top: "373"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "374"
  type: "ReLU"
  bottom: "373"
  top: "374"
}
layer {
  name: "375"
  type: "Convolution"
  bottom: "374"
  top: "375"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "376"
  type: "Convolution"
  bottom: "375"
  top: "376"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "377_bn"
  type: "BatchNorm"
  bottom: "376"
  top: "377"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "377"
  type: "Scale"
  bottom: "377"
  top: "377"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "378"
  type: "ReLU"
  bottom: "377"
  top: "378"
}
layer {
  name: "379"
  type: "Convolution"
  bottom: "378"
  top: "379"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "380"
  type: "Convolution"
  bottom: "379"
  top: "380"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "381_bn"
  type: "BatchNorm"
  bottom: "380"
  top: "381"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "381"
  type: "Scale"
  bottom: "381"
  top: "381"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "382"
  type: "ReLU"
  bottom: "381"
  top: "382"
}
layer {
  name: "383"
  type: "Convolution"
  bottom: "382"
  top: "383"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "384"
  type: "Convolution"
  bottom: "383"
  top: "384"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "385_bn"
  type: "BatchNorm"
  bottom: "384"
  top: "385"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "385"
  type: "Scale"
  bottom: "385"
  top: "385"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "386"
  type: "Eltwise"
  bottom: "385"
  bottom: "373"
  top: "386"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "387"
  type: "ReLU"
  bottom: "386"
  top: "387"
}
layer {
  name: "388"
  type: "Convolution"
  bottom: "387"
  top: "388"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "389"
  type: "Convolution"
  bottom: "388"
  top: "389"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "390_bn"
  type: "BatchNorm"
  bottom: "389"
  top: "390"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "390"
  type: "Scale"
  bottom: "390"
  top: "390"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "391"
  type: "ReLU"
  bottom: "390"
  top: "391"
}
layer {
  name: "392"
  type: "Convolution"
  bottom: "391"
  top: "392"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "393"
  type: "Convolution"
  bottom: "392"
  top: "393"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "394_bn"
  type: "BatchNorm"
  bottom: "393"
  top: "394"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "394"
  type: "Scale"
  bottom: "394"
  top: "394"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "395"
  type: "ReLU"
  bottom: "394"
  top: "395"
}
layer {
  name: "396"
  type: "Convolution"
  bottom: "395"
  top: "396"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "397"
  type: "Convolution"
  bottom: "396"
  top: "397"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "398_bn"
  type: "BatchNorm"
  bottom: "397"
  top: "398"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "398"
  type: "Scale"
  bottom: "398"
  top: "398"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "399"
  type: "Eltwise"
  bottom: "398"
  bottom: "386"
  top: "399"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "400"
  type: "ReLU"
  bottom: "399"
  top: "400"
}
layer {
  name: "401"
  type: "Convolution"
  bottom: "400"
  top: "401"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "402"
  type: "Convolution"
  bottom: "401"
  top: "402"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "403_bn"
  type: "BatchNorm"
  bottom: "402"
  top: "403"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "403"
  type: "Scale"
  bottom: "403"
  top: "403"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "404"
  type: "ReLU"
  bottom: "403"
  top: "404"
}
layer {
  name: "405"
  type: "Convolution"
  bottom: "404"
  top: "405"
  convolution_param {
    num_output: 728
    bias_term: false
    group: 728
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "406"
  type: "Convolution"
  bottom: "405"
  top: "406"
  convolution_param {
    num_output: 1024
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "407_bn"
  type: "BatchNorm"
  bottom: "406"
  top: "407"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "407"
  type: "Scale"
  bottom: "407"
  top: "407"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "408"
  type: "Pooling"
  bottom: "407"
  top: "408"
  pooling_param {
    pool: MAX
    kernel_h: 3
    kernel_w: 3
    stride_h: 2
    stride_w: 2
    pad_h: 1
    pad_w: 1
  }
}
layer {
  name: "409"
  type: "Convolution"
  bottom: "399"
  top: "409"
  convolution_param {
    num_output: 1024
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 2
    stride_w: 2
    dilation: 1
  }
}
layer {
  name: "410_bn"
  type: "BatchNorm"
  bottom: "409"
  top: "410"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "410"
  type: "Scale"
  bottom: "410"
  top: "410"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "411"
  type: "Eltwise"
  bottom: "408"
  bottom: "410"
  top: "411"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "412"
  type: "Convolution"
  bottom: "411"
  top: "412"
  convolution_param {
    num_output: 1024
    bias_term: false
    group: 1024
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "413"
  type: "Convolution"
  bottom: "412"
  top: "413"
  convolution_param {
    num_output: 1536
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "414_bn"
  type: "BatchNorm"
  bottom: "413"
  top: "414"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "414"
  type: "Scale"
  bottom: "414"
  top: "414"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "415"
  type: "ReLU"
  bottom: "414"
  top: "415"
}
layer {
  name: "416"
  type: "Convolution"
  bottom: "415"
  top: "416"
  convolution_param {
    num_output: 1536
    bias_term: false
    group: 1536
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "417"
  type: "Convolution"
  bottom: "416"
  top: "417"
  convolution_param {
    num_output: 2048
    bias_term: false
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "418_bn"
  type: "BatchNorm"
  bottom: "417"
  top: "418"
  batch_norm_param {
    use_global_stats: true
    eps: 9.99999974738e-06
  }
}
layer {
  name: "418"
  type: "Scale"
  bottom: "418"
  top: "418"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "419"
  type: "Convolution"
  bottom: "399"
  top: "419"
  convolution_param {
    num_output: 256
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "420"
  type: "Convolution"
  bottom: "419"
  top: "420"
  convolution_param {
    num_output: 1000
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "421"
  type: "Convolution"
  bottom: "419"
  top: "421"
  convolution_param {
    num_output: 2000
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "422"
  type: "Convolution"
  bottom: "418"
  top: "422"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 15
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "423"
  type: "Convolution"
  bottom: "422"
  top: "423"
  convolution_param {
    num_output: 490
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 15
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "424"
  type: "Convolution"
  bottom: "418"
  top: "424"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 15
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "425"
  type: "Convolution"
  bottom: "424"
  top: "425"
  convolution_param {
    num_output: 490
    bias_term: true
    group: 1
    pad_h: 0
    pad_w: 0
    kernel_h: 15
    kernel_w: 1
    stride_h: 1
    stride_w: 1
    dilation: 1
  }
}
layer {
  name: "426"
  type: "Eltwise"
  bottom: "423"
  bottom: "425"
  top: "426"
  eltwise_param {
    operation: SUM
  }
}

#========= RoI Proposal ============

layer {
   bottom: "420"
   top: "rpn_cls_score_reshape"
   name: "rpn_cls_score_reshape"
   type: "Reshape"
   reshape_param { shape { dim: 0 dim: 2 dim: -1 dim: 0 } }
}

layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param { shape { dim: 0 dim: 24 dim: -1 dim: 0 } }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "421"
  bottom: "im_info"
  top: "rois"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "'feat_stride': 16 \n'scales': !!python/tuple [4, 8, 16, 32]"
  }
}

#========= RCNN ============

layer {
  name: "roi_pool5"
  type: "PSROIPooling"
  bottom: "426"
  bottom: "rois"
  top: "427"
  roi_pooling_param {
    pooled_w: 7
    pooled_h: 7
    spatial_scale: 0.0625 # 1/16
  }
}

layer {
  name: "429"
  type: "Flatten"
  bottom: "427"
  top: "429"
}
layer {
  name: "430"
  type: "InnerProduct"
  bottom: "429"
  top: "430"
  inner_product_param {
    num_output: 2048
    bias_term: true
  }
}
layer {
  name: "431"
  type: "InnerProduct"
  bottom: "430"
  top: "431"
  inner_product_param {
    num_output: 20
    bias_term: true
  }
}
layer {
  name: "432"
  type: "InnerProduct"
  bottom: "430"
  top: "432"
  inner_product_param {
    num_output: 80
    bias_term: true
  }
}

